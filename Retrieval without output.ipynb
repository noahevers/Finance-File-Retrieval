{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993afe9-1f1c-437d-abca-cb1dc229e806",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install pdfminer.six\n",
    "!pip install unstructured\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06556657-fea2-4e19-a78b-0bc212908d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e2fec-82e2-42c4-8047-8b97eaf82410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text, chunk_size=1024):\n",
    "    tokens = word_tokenize(text)\n",
    "    chunks = [' '.join(tokens[i:i+chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f471125-3107-4b1d-b5aa-d494dd7c919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "def vectorize_chunks(chunks):\n",
    "    return model.encode(chunks)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75024c7-b9a2-4bf1-81d3-2531f42d6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def convert_pdf_to_text(pdf_path):\n",
    "    text = extract_text(pdf_path)\n",
    "    return text\n",
    "\n",
    "pdf_path = 'data/PepsiCo 10k.pdf'\n",
    "document_text = convert_pdf_to_text(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d537258-b436-4ed1-9037-4993e7df7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = create_chunks(document_text, chunk_size=1024)\n",
    "\n",
    "chunk_vectors = vectorize_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a6af0-af19-4712-8c41-065171037f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/financebench_sample_150.csv')\n",
    "\n",
    "# Display the first few rows to verify \n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f47f2-1cda-47d4-b7d5-d21d49829fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter queries for the specific document\n",
    "pepsico_queries = df[df['doc_name'] == 'PEPSICO_2022_10K']\n",
    "\n",
    "# Extract the queries and convert them to a list\n",
    "query_texts = pepsico_queries['question'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277ef19-d474-40f1-a0fc-677944bfd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vectors = model.encode(query_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d07236-d780-4c9b-8616-d80eda535017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def retrieve_top_k_chunks_for_all_queries(query_vectors, chunk_vectors, k=5):\n",
    "    all_top_chunks_indices = []\n",
    "    for query_vector in query_vectors:\n",
    "        similarities = cosine_similarity([query_vector], chunk_vectors)[0]\n",
    "        top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "        all_top_chunks_indices.append(top_k_indices)\n",
    "    return all_top_chunks_indices\n",
    "\n",
    "# Retrieve top-k chunks for all queries\n",
    "all_top_chunks_indices = retrieve_top_k_chunks_for_all_queries(query_vectors, chunk_vectors, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3daf4f-3825-433c-9066-07d3fe1f9175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_top_chunks_for_queries(queries, all_top_chunks_indices, chunks):\n",
    "    for i, query in enumerate(queries):\n",
    "        print(f\"Query: {query}\")\n",
    "        print(\"Top Chunks:\")\n",
    "        top_indices = all_top_chunks_indices[i]\n",
    "        for idx in top_indices:\n",
    "            print(f\"- Chunk {idx}:\\n{chunks[idx]}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Assuming 'query_texts' contains the text of your queries\n",
    "display_top_chunks_for_queries(query_texts, all_top_chunks_indices, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333db487-3994-4aa4-9de0-0751747a2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data setup\n",
    "scenarios = ['Fixed-size Chunk & Vectorize', 'Direct Access to Evidence']\n",
    "accuracies = [0, 80]\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(scenarios, accuracies, color=['red', 'blue', 'green'])\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Comparison of Retrieval Techniques')\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Adding text labels above the bars\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    plt.text(i, accuracy + 3, f'{accuracy}%', ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2ef43-a708-48a8-be2f-466d10b6d9f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the destination folder\n",
    "dest_folder = './data'\n",
    "Path(dest_folder).mkdir(exist_ok=True) # Ensure the destination folder exists\n",
    "\n",
    "def download_document(url, dest_folder, doc_name):\n",
    "    # Parse the URL to get a meaningful filename\n",
    "    filename = f\"{doc_name}.pdf\" if not doc_name.lower().endswith('.pdf') else doc_name\n",
    "\n",
    "    # Construct the full path where the file will be saved\n",
    "    filepath = Path(dest_folder) / filename\n",
    "\n",
    "    # Check if file already exists to avoid re-downloading\n",
    "    if not filepath.exists():\n",
    "        try:\n",
    "            # Make HTTP GET request to download the document\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status() # Ensure the request was succesful\n",
    "\n",
    "            # Write the document to a file\n",
    "            with open(filepath, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Downloaded {filename} to {dest_folder}\")\n",
    "        except requests.exceptions.RequestException as e: # Catch any request related errors\n",
    "            print(f\"Failed to download {url}. Error: {e}\")\n",
    "    else:\n",
    "        print(f\"{filename} already exists in {dest_folder}\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/financebench_sample_150.csv')\n",
    "\n",
    "# Download documents\n",
    "for _, row in df.iterrows():\n",
    "    if pd.notna(row['doc_link']) and pd.notna(row['doc_name']):\n",
    "        download_document(row['doc_link'], dest_folder, row['doc_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37959ed-2b71-4fd8-8efd-31594404c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Path object\n",
    "dest_path = Path(dest_folder)\n",
    "\n",
    "# List all PDF files in the destination folder\n",
    "downloaded_files = [file.name for file in dest_path.iterdir() if file.suffix == '.pdf']\n",
    "\n",
    "# Print the number of downloaded files\n",
    "print(f\"Number of downloaded files: {len(downloaded_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453d105-0f36-497c-b704-837babc881d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pdf_folder_path = 'data/reports'\n",
    "extracted_texts_path = os.path.join(pdf_folder_path, \"extracted_texts\")\n",
    "document_texts = {}\n",
    "\n",
    "# Ensure the directory for extracted texts exists\n",
    "os.makedirs(extracted_texts_path, exist_ok=True)\n",
    "\n",
    "for doc_name in df['doc_name'].unique():\n",
    "    pdf_path = os.path.join(pdf_folder_path, f\"{doc_name}.pdf\")\n",
    "    output_text_path = os.path.join(extracted_texts_path, f\"{doc_name}.txt\")\n",
    "\n",
    "    # Check if the PDF file exists before proceeding\n",
    "    if os.path.exists(pdf_path):\n",
    "    \n",
    "        # Check if the text has already been extracted and saved\n",
    "        if not os.path.exists(output_text_path):  # If not, extract and save\n",
    "            print(f\"Extracting text from {pdf_path}\")\n",
    "            document_text = convert_pdf_to_text(pdf_path)\n",
    "            with open(output_text_path, 'w', encoding='utf-8') as text_file:\n",
    "                text_file.write(document_text)\n",
    "            print(f\"Text saved to {output_text_path}\")\n",
    "        else:\n",
    "            print(f\"Text already extracted for {doc_name}\")\n",
    "    \n",
    "        # Load the extracted text into memory\n",
    "        with open(output_text_path, 'r', encoding='utf-8') as text_file:\n",
    "            document_texts[doc_name] = text_file.read()\n",
    "\n",
    "    else:\n",
    "        print(f\"PDF file does not exist: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8d042-c68d-4270-992c-f848606495d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_files_count = len([name for name in os.listdir(extracted_texts_path) if os.path.isfile(os.path.join(extracted_texts_path, name))])\n",
    "print(f\"Number of extracted text files: {extracted_files_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc4c586-23f0-442a-9ff0-c333780269ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['doc_name_encoded'] = encoder.fit_transform(df['doc_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e4e16-d008-4b16-8856-73700d313f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['question'] # Features\n",
    "y = df['doc_name_encoded'] # Labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5e725-e463-4d7a-b68f-a8c579cb21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e3bf6f-95f5-4628-901d-45070ff137c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf9062-c42b-400b-981e-8c7a889c3eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a218c0-1d0e-456a-aff1-9d9f27f9cb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Sample 5 random rows from the dataframe\n",
    "sample_questions = df.sample(n=5)\n",
    "\n",
    "# Print the selected questions and their doc_name\n",
    "for index, row in sample_questions.iterrows():\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Document Name: {row['doc_name']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd538aa-738f-410f-bded-ec7237219c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756158b-1a3d-434e-8f18-a923548af2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df['doc_name_encoded'] = encoder.fit_transform(df['doc_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3cda25-91dc-4ef8-8634-d081dc0364fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load Spacy's NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_keywords_with_ner(text):\n",
    "    # Process the text with Spacy NLP pipeline\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract entities identified by Spacy NER\n",
    "    keywords = [ent.text for ent in doc.ents if ent.label_ in [\"ORG\"]]\n",
    "\n",
    "    # Join keywords for TF-IDF vectorization\n",
    "    keywords_str = ' '.join(keywords)\n",
    "    return keywords_str\n",
    "\n",
    "# Apply keyword extraction on questions\n",
    "df['question_keywords'] = df['question'].apply(extract_keywords_with_ner)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['question_keywords'], df['doc_name_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating a TF-IDF Vectorizer to convert the questions into vectors\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Creating a Logistic Regression model\n",
    "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Creating a pipeline that first converts texts to vectors and then applies logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vectorizer),\n",
    "    ('logistic_regression', logistic_regression_model)\n",
    "])\n",
    "\n",
    "# Training the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Testing the model\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(f'Model Accuracy: {accuracy}')\n",
    "\n",
    "# Predicting document names\n",
    "predicted_doc_names_encoded = pipeline.predict(X_test)\n",
    "predicted_doc_names = encoder.inverse_transform(predicted_doc_names_encoded)\n",
    "\n",
    "# Example of displaying a few predictions\n",
    "for i, (question, prediction) in enumerate(zip(X_test[:5], predicted_doc_names[:5])):\n",
    "    print(f\"Question: {question}\\nPredicted Document Name: {prediction}\\n{'-'*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fb47a-aeec-4801-beb0-58e7d9108b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data setup\n",
    "scenarios = ['Baseline Model', 'NER Enhanced Model']\n",
    "accuracies = [13.333333, 13.333333]\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(scenarios, accuracies, color=['skyblue', 'red'])\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Comparison of Document Retrieval Models')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Adding text labels above the bars\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    plt.text(i, accuracy + 0.5, f'{accuracy: .2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c7ca1-c312-4374-bef7-986c8042ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb218b-83e1-45e8-bb5b-33d98abc4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0887e6-32a5-4cfe-b855-e6b98f7eec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba2b93-c948-4240-89dd-29aec7d8424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_doc_name(data):\n",
    "    try:\n",
    "        json_data = eval(data)\n",
    "        return f\"{json_data['company']}_{json_data['year']}_{json_data['financial_report_type'].replace('-', '')}.pdf\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting document name: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce952d0-e1a9-4f4e-865c-b6bb45c86f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['question']\n",
    "y = df['doc_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35356362-4591-4f6c-ad95-18259121cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_financial_report(question):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a financial analyst. Return information in JSON format with the relevant fields of company, year, and financial report type.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Which standard financial report (e.g., 10-K, 10-Q, 8-K) typically contains the information needed to answer this question: '{question}'?\"}\n",
    "            ]\n",
    "        )\n",
    "        # Extract and return the JSON formatted text from the response\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def format_doc_name(data):\n",
    "    try:\n",
    "        json_data = json.loads(data)\n",
    "        \n",
    "        # Normalize the company name\n",
    "        company = json_data.get('company', 'Unknown').replace(' ', '').replace('(', '').replace(')', '')\n",
    "\n",
    "        # Handle the year and report type formatting\n",
    "        year = json_data.get('year', 'Unknown')\n",
    "        if isinstance(year, list):\n",
    "            year = max(year, key=lambda x: int(x.replace('FY', '').replace('Q', '').split(' ')[0]))  # Get the latest year\n",
    "        year = str(year).replace('FY', '')  # Clean up any fiscal year notation\n",
    "\n",
    "        # Handle quarterly data\n",
    "        if 'Q' in year:\n",
    "            parts = year.split('Q')\n",
    "            year = parts[0] + 'Q' + parts[1]\n",
    "\n",
    "        financial_report_type = json_data.get('financial_report_type', '').replace('-', '')\n",
    "        \n",
    "        # Normalize the year to four digits\n",
    "        if len(year) == 2:  # Handle cases like '22' for 2022\n",
    "            year = '20' + year\n",
    "\n",
    "        return f\"{company}_{year}_{financial_report_type}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error in format_doc_name: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_predictions(questions):\n",
    "    predictions = []\n",
    "    for question in questions:\n",
    "        print(f\"Processing question: {question}\")\n",
    "        report_type = find_financial_report(question)\n",
    "        if report_type:\n",
    "            print(f\"Report type found: {report_type}\")\n",
    "            doc_name = format_doc_name(report_type)\n",
    "            if doc_name:\n",
    "                predictions.append(doc_name)\n",
    "                print(f\"Formatted document name: {doc_name}\")  # Print the document name here\n",
    "            else:\n",
    "                predictions.append('Formatting error')\n",
    "                print(\"Formatting error\")  # Indicate a formatting error\n",
    "        else:\n",
    "            predictions.append('Unknown')\n",
    "            print(\"Unknown report type\")  # Indicate that no report type was found\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d87502-a605-4579-a0a1-26f0da1abae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 random rows from the dataframe\n",
    "sample_questions = df.sample(n=30)\n",
    "\n",
    "# Print the selected questions and their doc_name\n",
    "for index, row in sample_questions.iterrows():\n",
    "    print(f\"Question: {row['question']}\")\n",
    "    print(f\"Document Name: {row['doc_name']}\\n\")\n",
    "\n",
    "# Extract only the question text to send to the prediction function\n",
    "question_texts = sample_questions['question'].tolist()\n",
    "actual_names = sample_questions['doc_name'].tolist()\n",
    "\n",
    "# Debug print to check what questions are being sent to the prediction function\n",
    "print(\"Questions being processed:\", question_texts)\n",
    "\n",
    "y_pred_sample = get_predictions(question_texts)\n",
    "print(\"Sample Predictions:\", y_pred_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa89fd-d175-4d42-8162-bef5db79cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual and Predicted document names\n",
    "actual_names = [\n",
    "    \"AMERICANEXPRESS_2022_10K\", \"MGMRESORTS_2023Q2_10Q\", \"JOHNSON&JOHNSON_2022Q4_EARNINGS\",\n",
    "    \"COSTCO_2021_10K\", \"LOCKHEEDMARTIN_2021_10K\", \"3M_2023Q2_10Q\",\n",
    "    \"CORNING_2020_10K\", \"Pfizer_2023Q2_10Q\", \"ADOBE_2022_10K\",\n",
    "    \"LOCKHEEDMARTIN_2022_10K\", \"PEPSICO_2023_8K_dated-2023-05-05\", \"ADOBE_2022_10K\",\n",
    "    \"AMD_2022_10K\", \"WALMART_2018_10K\", \"GENERALMILLS_2020_10K\",\n",
    "    \"PFIZER_2021_10K\", \"PEPSICO_2021_10K\", \"BOEING_2018_10K\",\n",
    "    \"VERIZON_2022_10K\", \"AMD_2022_10K\", \"3M_2023Q2_10Q\",\n",
    "    \"AMCOR_2023Q2_10Q\", \"VERIZON_2022_10K\", \"JPMORGAN_2021Q1_10Q\",\n",
    "    \"NIKE_2023_10K\", \"AES_2022_10K\", \"NIKE_2019_10K\",\n",
    "    \"PEPSICO_2023Q1_EARNINGS\", \"AMERICANEXPRESS_2022_10K\", \"BESTBUY_2023_10K\"\n",
    "]\n",
    "\n",
    "predicted_names = [\n",
    "    \"AmericanExpress_2022_10K\", \"MGM_H1 2023_10Q\", \"Johnson&JohnsonJnJ_2022_10K\", \n",
    "    \"Costco_2021_10K\", \"LockheedMartin_2021_10K\", \"3M_2023_10Q\",\n",
    "    \"Corning_Unknown_\", \"Pfizer_current or upcoming fiscal year_10K\", \"Adobe_2022_10K\",\n",
    "    \"LockheedMartin_2020-2022_10K\", \"Pepsico_2023_proxy statement (DEF 14A)\", \"Adobe_2022_10K\",\n",
    "    \"AMD_2022_10K\", \"Walmart_2018_10K\", \"GeneralMills_2020_\",\n",
    "    \"Pfizer_2019_10K\", \"PepsiCo_2021_10K\", \"Boeing_2018_10K\",\n",
    "    \"Verizon_2022_10K\", \"AMD_2022_10K\", \"3M_2023_10Q\",\n",
    "    \"AMCOR_2023_10Q\", \"Verizon_2022_10K\", \"JPMorganChase&Co.JPM_2021_10Q\",\n",
    "    \"Nike_2023_10K\", \"AESCorporation_2022_10K\", \"Nike_2019_10K\",\n",
    "    \"Pepsico_2023_8K\", \"AmericanExpress_2022_10K\", \"BestBuy_Unknown_\"\n",
    "]\n",
    "\n",
    "# Normalizing both lists to ensure case and spacing are not an issue\n",
    "actual_names_normalized = [name.replace(\" \", \"\").upper() for name in actual_names]\n",
    "predicted_names_normalized = [name.replace(\" \", \"\").upper() for name in predicted_names]\n",
    "\n",
    "# Calculating accuracy\n",
    "correct_predictions = sum(1 for actual, predicted in zip(actual_names_normalized, predicted_names_normalized) if actual == predicted)\n",
    "total_predictions = len(actual_names)\n",
    "accuracy_percentage = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "print(f\"Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Total Predictions: {total_predictions}\")\n",
    "print(f\"Accuracy: {accuracy_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fe17f-64e2-4d63-afec-335d137a305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_relevant_document(question, doc_names):\n",
    "    try:\n",
    "        doc_list = ', '.join(doc_names)  # List of all document names\n",
    "\n",
    "        prompt = f\"Given these financial documents: {doc_list}, which should be used to answer the question: '{question}'?\"\n",
    "        print(\"Prompt sent to model:\", prompt)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a financial analyst. Decide which financial document to use.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        predicted_report = response.choices[0].message.content.strip()\n",
    "        print(\"Predicted Report:\", predicted_report)\n",
    "\n",
    "        # Dynamic regex pattern based on document names\n",
    "        doc_names_regex = '|'.join(re.escape(doc) for doc in doc_names)\n",
    "        pattern = rf'({doc_names_regex})'\n",
    "        match = re.search(pattern, predicted_report)\n",
    "        matched_document = match.group(1) if match else None\n",
    "\n",
    "        if matched_document:\n",
    "            print(\"Matched Document:\", matched_document)\n",
    "        else:\n",
    "            print(\"No matching document found.\")\n",
    "\n",
    "        return matched_document\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while finding the relevant document: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_document_matching(df, num_samples=30):\n",
    "    samples = df.sample(n=num_samples)\n",
    "    \n",
    "    # Collect all document names from the DataFrame to provide as options\n",
    "    all_doc_names = df['doc_name'].unique().tolist()\n",
    "\n",
    "    for index, sample in samples.iterrows():\n",
    "        question = sample['question']\n",
    "        actual_doc_name = sample['doc_name'].strip()\n",
    "\n",
    "        predicted_doc_name = find_relevant_document(question, all_doc_names)  # Pass all possible documents\n",
    "\n",
    "        is_correct = predicted_doc_name == actual_doc_name\n",
    "        status = \"Correct\" if is_correct else \"Incorrect\"\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Predicted Document: {predicted_doc_name}\")\n",
    "        print(f\"Actual Document: {actual_doc_name}\")\n",
    "        print(f\"Status: {status}\")\n",
    "\n",
    "# Example usage\n",
    "num_questions_to_test = 30  # Adjust the number of tests as needed\n",
    "test_document_matching(df, num_questions_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e97a5-9514-48f4-b4fe-c2a921951b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 24/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503bb7ea-c584-4aa4-844e-d82d2874bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_document(question, doc_names):\n",
    "    try:\n",
    "        doc_list = ', '.join(doc_names)  # List of all document names\n",
    "\n",
    "        prompt = f\"Given these financial documents: {doc_list}, which should be used to answer the question: '{question}'?\"\n",
    "        print(\"Prompt sent to model:\", prompt)\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a financial analyst. Decide which financial document to use.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        predicted_report = response.choices[0].message.content.strip()\n",
    "        print(\"Predicted Report:\", predicted_report)  # Debugging output\n",
    "\n",
    "        # Dynamic regex pattern to extract document names and their years/quarters\n",
    "        doc_names_regex = '|'.join(re.escape(doc) for doc in doc_names)\n",
    "        pattern = rf'({doc_names_regex})'\n",
    "        matches = re.findall(pattern, predicted_report)\n",
    "        \n",
    "        if matches:\n",
    "            # Parse years and sort documents by recency\n",
    "            def extract_year(doc):\n",
    "                match = re.search(r'(\\d{4})', doc)\n",
    "                return int(match.group(1)) if match else 0\n",
    "\n",
    "            sorted_docs = sorted(matches, key=extract_year, reverse=True)\n",
    "            matched_document = sorted_docs[0] if sorted_docs else None\n",
    "            print(\"Matched Document:\", matched_document)\n",
    "        else:\n",
    "            matched_document = None\n",
    "            print(\"No matching document found.\")\n",
    "\n",
    "        return matched_document\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while finding the relevant document: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_document_matching(df, num_samples=30):\n",
    "    samples = df.sample(n=num_samples)\n",
    "    all_doc_names = df['doc_name'].unique().tolist()\n",
    "\n",
    "    for index, sample in samples.iterrows():\n",
    "        question = sample['question']\n",
    "        predicted_doc_name = find_relevant_document(question, all_doc_names)\n",
    "        actual_doc_name = sample['doc_name'].strip()\n",
    "\n",
    "        is_correct = predicted_doc_name == actual_doc_name\n",
    "        status = \"Correct\" if is_correct else \"Incorrect\"\n",
    "\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Predicted Document: {predicted_doc_name}\")\n",
    "        print(f\"Actual Document: {actual_doc_name}\")\n",
    "        print(f\"Status: {status}\")\n",
    "\n",
    "# Example usage\n",
    "num_questions_to_test = 30  # Adjust the number of tests as needed\n",
    "test_document_matching(df, num_questions_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8d252-492c-4dae-b431-c55d099f70c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 25/30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
